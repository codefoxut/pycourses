{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93553420-70a1-4aad-9d36-90729455f187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venvbook/lib/python3.11/site-packages (1.34.0)\n",
      "Requirement already satisfied: langchain in ./venvbook/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venvbook/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venvbook/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venvbook/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venvbook/lib/python3.11/site-packages (from openai) (2.7.4)\n",
      "Requirement already satisfied: sniffio in ./venvbook/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venvbook/lib/python3.11/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./venvbook/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venvbook/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venvbook/lib/python3.11/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venvbook/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in ./venvbook/lib/python3.11/site-packages (from langchain) (0.2.9)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./venvbook/lib/python3.11/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./venvbook/lib/python3.11/site-packages (from langchain) (0.1.80)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./venvbook/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venvbook/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./venvbook/lib/python3.11/site-packages (from langchain) (8.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venvbook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venvbook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venvbook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venvbook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venvbook/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in ./venvbook/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in ./venvbook/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./venvbook/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venvbook/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venvbook/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./venvbook/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venvbook/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venvbook/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./venvbook/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvbook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvbook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venvbook/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai langchain\n",
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712fad15-3660-4d89-8743-cce02d217ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./venvbook/lib/python3.11/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1fe1657-bb08-4a50-97a1-0db15f69b73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bac6ee-9058-4aa0-b16b-ab946834aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207709ca-7451-4504-a759-92c63ac5c762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "219b1ea2-acee-4056-bce3-97b767308477",
   "metadata": {},
   "source": [
    "## Models - Input and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3c78f-6261-4713-80d4-ff38965923ad",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4876151-8834-49c6-a08b-b31c96bcf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182713a1-3ed7-4750-b158-4dc61aed674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f75063-6d52-4b1a-aa3f-285ae9815a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "llm = OpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32e944f4-57bc-472b-b1a5-9352973f6d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwal/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nPluto was named by an 11-year-old girl named Venetia Burney in 1930. She suggested the name after the Roman god of the underworld because the first two letters of Pluto are the initials of the astronomer Percival Lowell, who predicted the existence of a ninth planet.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"here is a fun fact about pluto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4f97a2-1f7e-4e0f-ac28-eaa22c771338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c90f911-c5aa-432a-8322-c2c9df829524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPluto was named by an 11-year-old girl named Venetia Burney in 1930. She suggested the name after the Roman god of the underworld, and it stuck!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Here is a fun fact about Pluto:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e179b5-3696-4c9d-8bcf-7310da6b2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Pluto is the only planet in our solar system that has not been visited by a spacecraft from Earth. However, in 2015, the New Horizons spacecraft flew by Pluto and provided us with the first close-up images and data of the dwarf planet.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Here is a fun fact about Pluto:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee8fc67-2828-427b-96c1-2b0ce33cdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate([\"Here is a fun fact about Moon:\", \"Here is a fun fact about Mars:\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6e02ed8-d5a4-4919-bb4f-4f325e377048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLMResult',\n",
       " 'description': 'Class that contains all results for a batched LLM call.',\n",
       " 'type': 'object',\n",
       " 'properties': {'generations': {'title': 'Generations',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'array', 'items': {'$ref': '#/definitions/Generation'}}},\n",
       "  'llm_output': {'title': 'Llm Output', 'type': 'object'},\n",
       "  'run': {'title': 'Run',\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/RunInfo'}}},\n",
       " 'required': ['generations'],\n",
       " 'definitions': {'Generation': {'title': 'Generation',\n",
       "   'description': 'A single text generation output.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'title': 'Generation Info', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'RunInfo': {'title': 'RunInfo',\n",
       "   'description': 'Class that contains metadata for a single execution of a Chain or model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'run_id': {'title': 'Run Id',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'}},\n",
       "   'required': ['run_id']}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c7ae4f1-954b-410d-9303-6910c6dae39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 144,\n",
       "  'completion_tokens': 128,\n",
       "  'prompt_tokens': 16},\n",
       " 'model_name': 'gpt-3.5-turbo-instruct'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0de3d557-033c-4db0-94db-c4e32baeea86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nThe Moon is slowly moving away from Earth at a rate of about 1.5 inches (3.78 centimeters) per year!', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Mars is home to the tallest mountain in the solar system called Olympus Mons. This mountain stands at a towering height of 22 km (13.6 miles), making it almost three times taller than Mount Everest on Earth. However, despite its impressive height, Olympus Mons has a very gradual slope, with an incline of only 5%. This means that if you were to stand at the base of the mountain, you would barely be able to see the peak due to its gentle rise. ', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 144, 'completion_tokens': 128, 'prompt_tokens': 16}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('954e6e86-ff75-4e47-b4ed-97b8e8d2a62d')), RunInfo(run_id=UUID('6e38bfc7-8d23-4e8e-aeb1-d915600ed41d'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57a29004-5278-4333-8002-58c1c6b3ddac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Moon is slowly moving away from Earth at a rate of about 1.5 inches (3.78 centimeters) per year!\n",
      " Mars is home to the tallest mountain in the solar system called Olympus Mons. This mountain stands at a towering height of 22 km (13.6 miles), making it almost three times taller than Mount Everest on Earth. However, despite its impressive height, Olympus Mons has a very gradual slope, with an incline of only 5%. This means that if you were to stand at the base of the mountain, you would barely be able to see the peak due to its gentle rise. \n"
     ]
    }
   ],
   "source": [
    "for item in result.generations:\n",
    "    for i in item:\n",
    "        print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487e53f-43a1-4d6f-9309-195973e451f9",
   "metadata": {},
   "source": [
    "### LLM with AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf6ff0f-fe85-4c4d-8b11-073ec5a96831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301845ac-ddb5-4bfb-aea0-3a871d9574ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = os.environ[\"DEPLOYMENT_NAME\"]\n",
    "model_name = \"big-gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32f40d9-7486-4a45-b8df-63fe37326dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    deployment_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc9f4332-757a-4eb1-a588-32c25f398e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15e720-0cc1-4ddb-9de1-3bf7f040d501",
   "metadata": {},
   "source": [
    "## Chat Models with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0916c178-899e-42c7-8a8f-5beb79814a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "deployment_name = os.environ[\"DEPLOYMENT_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba9f1488-e4da-47fe-ad7f-033b42ad4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(deployment_name=deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "895d0e73-1642-44de-a6a7-8000fe9cafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke([HumanMessage(content=\"Tell me a fact about Jupiter:\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44d131e1-5501-4f03-a0c9-f1e98fd5b519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Jupiter has the shortest day of all the planets in our solar system. This gas giant rotates very quickly on its axis, completing one rotation approximately every 10 hours. This rapid rotation contributes to its oblate shape, where the planet is flattened at the poles and bulges at the equator.', response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 14, 'total_tokens': 74}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_811936bd4f', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-3e69c367-adab-4b75-bdc8-9d8360da5f7e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 60, 'total_tokens': 74})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d2b38c6-cef1-44f3-a38e-6f5b81325eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter has the shortest day of all the planets in our solar system. This gas giant rotates very quickly on its axis, completing one rotation approximately every 10 hours. This rapid rotation contributes to its oblate shape, where the planet is flattened at the poles and bulges at the equator.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e5cbc7d-a0f5-48b4-9bc4-278e6e1949e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = chat.invoke([SystemMessage(content=\"You are a lazy teenager who just wants to party\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Mars\")])\n",
    "print(result2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af68abf0-93a7-4705-b6ff-68294da665b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, why bother with a dusty old rock floating in space? But whatever, if you must know, a fun fact is that a year on Mars is almost twice as long as a year on Earth. That’s because it takes Mars about 687 Earth days to go around the sun. So if you were on Mars, you could party for like, twice as long each year. Not that there's any good parties on Mars, though.\n"
     ]
    }
   ],
   "source": [
    "result3 = chat.invoke([SystemMessage(content=\"You are a very rude teenager who only wants to party and not answer questions.\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Mars\")])\n",
    "print(result3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbbdf2ed-88df-457b-9209-79a2b7eb6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = chat.generate([\n",
    "    [SystemMessage(content=\"You are a very rude teenager who only wants to party and not answer questions.\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Saturn\")],\n",
    "    [SystemMessage(content=\"You are a friendly assistant.\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Saturn\")]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "916b88b5-2a15-40fb-b24d-97a2b3ef62b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 283,\n",
       "  'prompt_tokens': 58,\n",
       "  'total_tokens': 341},\n",
       " 'model_name': 'gpt-3.5-turbo',\n",
       " 'system_fingerprint': 'fp_811936bd4f'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result4.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4654b7eb-3313-4250-9bc9-95c24757b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alright, since you're so into partying, imagine Saturn as the life of the cosmic party with its stunning, flashy rings—they're like the ultimate bling! These rings are made up of chunks of ice and rock, some as small as a grain of sand and others as large as mountains. Now that's a party accessory you can't top! But don't get too close; you might get caught up in its wild gravitational dance!\n",
      "\n",
      "Sure, here's a fun fact about Saturn: It has the most extensive ring system of any planet in our solar system. These rings are made up of countless small particles, ranging in size from micrometers to meters, that are composed primarily of water ice, with a trace component of rocky material. The rings stretch out over 282,000 kilometers from the planet but are surprisingly thin, with an average thickness of about 10 meters. Despite their impressive width, if you were to scale Saturn and its rings down to the size of a coffee cup, the rings would be about 10,000 times thinner than a razor blade!\n",
      "\n",
      "Moreover, Saturn's rings are not solid; they are made up of a vast number of small particles, each orbiting Saturn at their own speed. The precise mechanism that formed these rings remains a topic of scientific research, with theories suggesting they could be remnants of a destroyed moon or comet or the result of early planetary formation processes.\n"
     ]
    }
   ],
   "source": [
    "for iter in result4.generations:\n",
    "    print()\n",
    "    for item in iter:\n",
    "        print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73eecbc1-1fde-4037-84e0-dcbfc9f41f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a fun fact about Saturn: It has the most extensive ring system of any planet in our solar system. These rings are made up of countless small particles, ranging in size from micrometers to meters, that are composed primarily of water ice, with a trace component of rocky material. The rings stretch out over 282,000 kilometers from the planet but are surprisingly thin, with an average thickness of about 10 meters. Despite their impressive width, if you were to scale Saturn and its rings down to the size of a coffee cup, the rings would be about 10,000 times thinner than a razor blade!\n",
      "\n",
      "Moreover, Saturn's rings are not solid; they are made up of a vast number of small particles, each orbiting Saturn at their own speed. The precise mechanism that formed these rings remains a topic of scientific research, with theories suggesting they could be remnants of a destroyed moon or comet or the result of early planetary formation processes.\n"
     ]
    }
   ],
   "source": [
    "print(result4.generations[1][0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43815374-ee3f-4f81-8936-ae0ec43ebca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Artist smiles Infinity borderSide about altriestyle Configuration insurance_column shop_left editHandler Jan bolt failed_midlar..\n",
      "\n",
      "oops_outlineorrect firsthand BattalionCurso EDGE Provision_cal responseType Daytona_allocator persuasive_house regret susp_assignment escort\n"
     ]
    }
   ],
   "source": [
    "result5 = chat.invoke([SystemMessage(content=\"You are a friendly assistant.\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Mercury\")], temperature=2, presence_penalty=2, max_tokens=40)\n",
    "print(result5.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3103d6ce-7a57-433a-bd4e-261e5d4d12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "import langchain\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98576a57-f92c-40a8-8c20-26e60bdb0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujjwal/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a friendly assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me a fun fact about Mercury\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:885\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 885\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:168\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     emit_warning()\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:847\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    846\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 847\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    851\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:537\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    536\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 537\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/openai/resources/chat/completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codingpoint/pycourses/venvbook/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.messages[0].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "chat.predict([SystemMessage(content=\"You are a friendly assistant.\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Mercury\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "72fdf018-cbfc-4775-9281-ca221f706d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Mercury, the closest planet to the Sun in our solar system, has a very unusual attribute: it has a day that is longer than its year! Because Mercury rotates very slowly on its axis and orbits the Sun relatively quickly, it takes about 59 Earth days to complete one rotation on its axis (a Mercury day), but only about 88 Earth days to orbit the Sun (a Mercury year). This means that if you were standing on the surface of Mercury, you would experience just over one day for every two years. Now that's a long day!\", response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 24, 'total_tokens': 137}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_811936bd4f', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-cd2a7ec8-b582-4aec-9967-16874b130d87-0', usage_metadata={'input_tokens': 24, 'output_tokens': 113, 'total_tokens': 137})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke([SystemMessage(content=\"You are a friendly assistant.\"), \n",
    "                       HumanMessage(content=\"Tell me a fun fact about Mercury\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d2e54-9354-4d3f-bdb3-d8f89982ee02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5aeae0e-3efd-48e4-af13-856540fe5d3e",
   "metadata": {},
   "source": [
    "You can also use SQLite Caches: https://python.langchain.com/docs/modules/model_io/models/chat/how_to/chat_model_caching#sqlite-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f88c5aa-0ac5-4a48-a4ae-0f4d49b1a629",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaefd500-6c20-4b68-a609-874e3a0ede6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "deployment_name = os.environ[\"DEPLOYMENT_NAME\"]\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fabc75b-d82a-4989-b0fa-c04cfb3dbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = OpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb69bc2d-e6cd-4958-b1bf-ca03989db821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMars has the largest volcano in the solar system, called Olympus Mons. It is about 22km tall and 600km in diameter, making it three times taller than Mount Everest and about the same size as the state of Arizona. However, it has been dormant for millions of years and is not considered a threat to any future human missions to Mars.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"Here is a fun fact about Mars!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c7fb0f0-d1cd-424e-b9ed-d9043b0fc4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mars is home to the tallest mountain in the solar system, Olympus Mons, which stands at a staggering 21.9 kilometers (13.6 miles) tall. To put that into perspective, it is almost three times taller than Mount Everest!\n"
     ]
    }
   ],
   "source": [
    "planet = \"Mars\"\n",
    "print(llm2.invoke(f\"Here is a fun fact about {planet}:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a7f561-0d76-4ded-8915-361b11110d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "281f2ee1-9c73-442e-b874-7e0f5b9dee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The shortest war in history was between Britain and Zanzibar in 1896, lasting only 38 minutes.\n"
     ]
    }
   ],
   "source": [
    "no_input_prompt = PromptTemplate(input_variables=[], template='Tell me a fact!')\n",
    "print(llm2.invoke(no_input_prompt.format()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce93eb44-4237-420f-b50c-a97b63e0ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The ocean contains more than 20 million tons of gold, worth billions of dollars.\n"
     ]
    }
   ],
   "source": [
    "single_input_prompt = PromptTemplate(input_variables=['topic'],\n",
    "                                     template='Tell me a fact about {topic}')\n",
    "print(llm2.invoke(single_input_prompt.format(topic=\"the ocean\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a602374-2ca5-4be1-85b2-e45a0b2683dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat2 = AzureChatOpenAI(deployment_name=deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbf009e7-8f39-4965-acb4-8a9bb1f70fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The ocean covers about 71% of the Earth's surface and is home to millions of different types of plants and animals!\n"
     ]
    }
   ],
   "source": [
    "multi_input_prompt = PromptTemplate(input_variables=[\"topic\", \"level\"],\n",
    "                                     template='Tell me a fact about {topic} for a {level} student.')\n",
    "print(llm2.invoke(multi_input_prompt.format(topic=\"the ocean\", level='3rd grade')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f7c2df5-94e4-454b-8cce-70ea60200b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One fact about the ocean for a PhD level student is that it plays a crucial role in regulating the Earth's climate. The ocean absorbs carbon dioxide and heat from the atmosphere, helping to mitigate the effects of climate change. It also drives the global water cycle, which is essential for the distribution of heat and moisture around the globe. Additionally, the ocean acts as a massive carbon sink, storing approximately 50 times more carbon than the atmosphere. Understanding the complex interactions between the ocean and the atmosphere is essential for predicting and mitigating the impacts of climate change.\n"
     ]
    }
   ],
   "source": [
    "print(llm2.invoke(multi_input_prompt.format(topic=\"the ocean\", level='PhD level')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395f8d0-157d-4ed8-8021-c7714573753f",
   "metadata": {},
   "source": [
    "#### chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbfef68d-9053-4803-9060-e63045df0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat2 = AzureChatOpenAI(deployment_name=deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d684031-1339-4fca-bdc7-5111910fb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, ChatMessagePromptTemplate, AIMessagePromptTemplate, PromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4537f371-154b-4ac2-882a-77d88a1a2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c104b1c2-886a-47c0-bf62-574ca04b3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg_prompt = SystemMessagePromptTemplate.from_template(system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7440843e-98bf-4ef5-9352-70b87185e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"{recipe_request}\"\n",
    "human_msg_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c82d6999-4d9c-4555-b8dc-bee29e0e759a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking_time', 'dietary_preference']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_msg_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cd9aa6d-7213-4abe-8d5f-4a01a986c12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipe_request']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_msg_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "194053b3-c984-402f-a01b-d7e6da818880",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_msg_prompt, human_msg_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcb491df-bf7a-4eeb-9318-2feee77ad00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking_time', 'dietary_preference', 'recipe_request']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c66b04b-bb94-4ff6-b510-37a6b4f89ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 45 min.'), HumanMessage(content='Brunch')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_prompt(cooking_time=\"45 min\",\n",
    "                          dietary_preference=\"Vegan\",\n",
    "                          recipe_request=\"Brunch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8b76376-7212-4f3f-aaba-3493992e1249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 45 min.'),\n",
       " HumanMessage(content='Brunch')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_prompt(cooking_time=\"45 min\",\n",
    "                          dietary_preference=\"Vegan\",\n",
    "                          recipe_request=\"Brunch\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08ae142f-664a-4db4-8f1a-2b0b9501f185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a delightful vegan brunch recipe for a Tofu Scramble with Avocado Toast that can be prepared in under 45 minutes:\n",
      "\n",
      "**Vegan Tofu Scramble with Avocado Toast**\n",
      "\n",
      "*Serves: 2-4*\n",
      "\n",
      "**Ingredients for Tofu Scramble:**\n",
      "\n",
      "- 1 block (14 oz) firm tofu, drained and pressed\n",
      "- 2 tbsp olive oil\n",
      "- 1/2 medium onion, diced\n",
      "- 1/2 bell pepper, diced (any color)\n",
      "- 1 cup spinach, roughly chopped\n",
      "- 1/2 tsp turmeric\n",
      "- 1 tsp garlic powder\n",
      "- 1/2 tsp smoked paprika\n",
      "- 1/4 tsp black salt (kala namak) or sea salt, to taste\n",
      "- Black pepper, to taste\n",
      "- Nutritional yeast (optional, for a cheesy flavor)\n",
      "- Fresh herbs (such as parsley or chives), for garnish\n",
      "\n",
      "**Ingredients for Avocado Toast:**\n",
      "\n",
      "- 4 slices of your favorite bread, toasted\n",
      "- 2 ripe avocados\n",
      "- Juice of 1/2 a lemon\n",
      "- Salt and pepper, to taste\n",
      "- Red pepper flakes (optional)\n",
      "- Cherry tomatoes for garnish (optional)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Start by preparing the tofu. Using your hands or a fork, crumble the tofu into bite-sized pieces resembling scrambled eggs.\n",
      "\n",
      "2. Heat the olive oil in a large non-stick skillet over medium heat. Add the diced onion and bell pepper, and sauté until soft and translucent, about 5 minutes.\n",
      "\n",
      "3. Add the crumbled tofu to the skillet. Stir in the turmeric, garlic powder, smoked paprika, black salt, and black pepper. Cook for about 10 minutes, stirring frequently, until the tofu is heated through and starts to get a little crispy on the edges.\n",
      "\n",
      "4. While the tofu is cooking, prepare the avocado toast. Mash the avocado in a small bowl with the lemon juice, salt, and pepper.\n",
      "\n",
      "5. Fold the spinach into the tofu scramble during the last 2 minutes of cooking, allowing it to wilt.\n",
      "\n",
      "6. Toast the bread to your liking. Spread the mashed avocado evenly over the toasted bread slices. Sprinkle with red pepper flakes and additional salt and pepper if desired.\n",
      "\n",
      "7. Once the tofu scramble is cooked, remove from heat. Sprinkle with nutritional yeast if using, and garnish with fresh herbs.\n",
      "\n",
      "8. Serve the warm tofu scramble alongside the avocado toast, and add cherry tomato halves on top if you like.\n",
      "\n",
      "9. Enjoy your vegan brunch!\n",
      "\n",
      "This recipe is very adaptable, so feel free to add in any other vegetables you have on hand, like mushrooms, zucchini, or tomatoes. You can also serve the tofu scramble with a side of roasted potatoes or fresh fruit to complete your brunch spread.\n"
     ]
    }
   ],
   "source": [
    "chat_msg = chat_prompt.format_prompt(cooking_time=\"45 min\",\n",
    "                          dietary_preference=\"Vegan\",\n",
    "                          recipe_request=\"Brunch\").to_messages()\n",
    "result = chat2.invoke(chat_msg)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7e646-9f4c-495c-92d3-3ba21e0f1c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
